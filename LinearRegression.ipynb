
{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Programming Exercise 1: Linear Regression\n",
    "\n",
    "This notebook is based on the [Coursera Machine Learning](https://www.coursera.org/learn/machine-learning) course by Andrew Ng. We believe it is the best course for people who are keen to make their first steps in Machine Learning.\n",
    "\n",
    "The course is taught in Octave / MATLAB, but Python is much more broadly used in the industry and academia. The Python code in this tutorial is based on some work by [David Kaleko](https://github.com/kaleko). \n",
    "\n",
    "Explanations of the model, and any mistakes herein, are of our own doing. If you're unsure about anything, feel free to ask us during the session!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Start by importing the libraries with the necessary functionality for ML\n",
    "# in Python and for vanilla ML, that almost always includes NumPy\n",
    "\n",
    "# the following line allows us to output plots in the notebook\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt # plotting library"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The task of supervised learning is to train a model to predict a label $y$ given an input $x = (x_1, x_2, \\dotsc, x_n)$, where $n$ is the number of input features. For example, $y$ as the target variable can be the price of a house, and $x$ is a vector consisting of $n$ features of the house, such as: the number of bedrooms, proximity to public transport, age of house, etc... Our task then is to build a model that can take these input features of the house and predict the price that the house will be sold at. \n",
    "\n",
    "To train a supervised machine-learning algorithm, you would need a bunch of labelled examples $\\{(x^{(1)}, y^{(1)}), (x^{(2)}, y^{(2)}), \\dotsc, (x^{(m)}, y^{(m)})\\}$ which we call the **training data**, where $m$ is the number of datapoints. Using the above example, we would need a dataset containing the sold price of $m$ houses, along with the $n$ features for each of those houses.\n",
    "\n",
    "A supervised machine learning model typically assumes that there exists a mapping from inputs to outputs, which we can represent as\n",
    "\n",
    "$$ y = h_{\\theta}(x) + \\epsilon $$\n",
    "\n",
    "where $h_{\\theta}(x)$ is called a hypothesis and $\\epsilon$ (pronounced $epsilon$) is a noise term that represents any error that cannot be accounted for by the hypothesis $h$. $\\theta$ (pronounced $theta$) are the model parameters which we will want to tweak to make the predictions as accurate as possible (you will see what we mean in a bit).\n",
    "\n",
    "Linear regression is one of the simplest ML models. It assumes that the mapping from $x$ to $y$ is linear in parameters (i.e. it is composed of the sum of model parameters):\n",
    "\n",
    "$$ y = \\theta_0 + \\theta_1 x_1 + \\cdots + \\theta_n x_n + \\epsilon$$\n",
    "\n",
    "where $\\theta_1 x_1$ is $\\theta_1$ times $x_1$, $x = (x_1, x_2, \\dotsc, x_n)$ are the input variables for one data sample and $\\theta = (\\theta_0, \\theta_1, \\theta_2, \\dotsc, \\theta_n)$ are the model parameters to be learned.\n",
    "\n",
    "Intuitively, the parameter $\\theta_n$ in $\\theta_n x_n$ denotes the slope or how much $y$ is expected to change with each unit change in $x_n$. $\\theta$ can be either positive or negative, e.g. house price is expected to increase by $\\theta_{bed}$ for every extra bedroom, or it is expected to decreased by $\\theta_{transport}$ for every additional meter from a public transport link.\n",
    "\n",
    "Note that we have a $\\theta_n$ for each $x_n$ but we have one more additional term, $\\theta_0$, which we call the bias. This can be interpreted as the *intercept* term, or what $y$ is expected to be in absence of any additional knowledge about $x_n$.\n",
    "\n",
    "It is important to note that linearity is in parameters $\\theta$, *not* the variables $x$. This means that a linear regression permits polynomials or interactions of original variables, for example:\n",
    "\n",
    "$$ y = \\theta_0 + \\theta_1 x_1 + \\theta_2 x_1^2 + \\theta_3 x_2 + \\theta_4 x_1x_2 + \\epsilon$$\n",
    "\n",
    "In theory, this permits the model to learn non-linear relationships between $y$ and (non-transformed) $x$. However, these non-linear relationships and interaction effects are not known a-priori. In practice, we tend to use the simplest model with no higher-order terms.\n",
    "\n",
    "In general linear regression is not the most predictive model out of the box. Nonetheless, it is a very useful baseline in many contexts (and in industry) and benefits from the ease of interpretation and development."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Linear regression with one variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start with a simple linear regression model in one variable. You will implement linear regression with one\n",
    "variable to predict profits for a food truck. \n",
    "\n",
    "Suppose you are the CEO of a restaurant franchise and are considering different cities for opening a new outlet.\n",
    "The chain already has trucks in various cities and you have a dataset containing the profits and population size for $m$ cities.\n",
    "The training data can be represented as $\\{(x^{(1)}, y^{(1)}), (x^{(2)}, y^{(2)}), \\dotsc, (x^{(m)}, y^{(m)})\\}$ where $x^{(i)}$ represents the population (in 10k increment) and $y^{(i)}$ represents profit (in \\$10k increment) of the $i$th city.\n",
    "\n",
    "You would like to use this data to help you select which city to expand to next.\n",
    "\n",
    "The file *ex1data1.txt* contains the dataset for our linear regression problem. The first column is the population of a city and the second column is the profit of a food truck in that city. A negative value for profit indicates a\n",
    "loss.\n",
    "\n",
    "\n",
    "Consider output of the hypothesis $h_{\\theta}$\n",
    "\n",
    "$$ h_{\\theta}(x) = \\theta_0 + \\theta_1 x. $$\n",
    "\n",
    "\n",
    "Let's try to interpret this equation. It assumes that the relationship between $y$ and $x$ is linear -- meaning that if we increase $x$ by 1, $h_{\\theta}(x)$ will increase by $\\theta_1$ and if we increase $x$ by $47.1$, $h_{\\theta}(x)$ will increase $47.1 \\cdot \\theta_1$, or $47.1$ times $\\theta_1$. The term $\\theta_0$ allows to shift the linear dependency between $h_{\\theta}(x)$ and $x$ by a constant. When $x = 0$, $h_{\\theta}(x)$ predicts $y$ to be exactly equal to $\\theta_0$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Loading the Data\n",
    "\n",
    "In machine learning, we usually represent the data $\\{(x^{(1)}, y^{(1)}), (x^{(2)}, y^{(2)}), \\dotsc, (x^{(m)}, y^{(m)})\\}$ as a matrix $X$ and a vector $y$ where\n",
    "\\begin{align}\n",
    "    X &=\n",
    "        \\begin{bmatrix}\n",
    "            x_1^{(1)} & x_2^{(1)} & \\cdots & x_n^{(1)} \\\\\n",
    "            x_1^{(2)} & x_2^{(2)} & \\cdots & x_n^{(2)} \\\\\n",
    "            \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "            x_1^{(m)} & x_2^{(m)} & \\cdots & x_n^{(m)} \\\\\n",
    "        \\end{bmatrix} &&\\text{and}\\\\\n",
    "    y &= \n",
    "        \\begin{bmatrix}\n",
    "            y^{(1)} \\\\\n",
    "            y^{(2)} \\\\\n",
    "            \\vdots \\\\\n",
    "            y^{(m)}\n",
    "        \\end{bmatrix}.\n",
    "\\end{align}\n",
    "\n",
    "In the one dimensional case the matrix $X$ is just\n",
    "\\begin{align}\n",
    "    X &=\n",
    "        \\begin{bmatrix}\n",
    "            x^{(1)} \\\\\n",
    "            x^{(2)} \\\\\n",
    "            \\vdots \\\\\n",
    "            x^{(m)}\n",
    "        \\end{bmatrix}.\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "datafile = 'data/ex1data1.txt'\n",
    "cols = np.loadtxt(datafile, delimiter=',', usecols=(\n",
    "    0, 1), unpack=True)  # Read in comma separated data\n",
    "\n",
    "# Form the usual \"X\" matrix and \"y\" vector\n",
    "X = np.transpose(np.array(cols[:-1]))\n",
    "y = np.transpose(np.array(cols[-1:]))\n",
    "m = y.size  # number of training examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Plotting the Data\n",
    "\n",
    "Before starting on any task, it is often useful to understand the data by visualizing it. For this dataset, you can use a scatter plot to visualize the data, since it has only two properties to plot (profit and population).\n",
    "\n",
    "We focus on the inner workings of machine learning algorithms and therefore you do not need to fill in anything for plotting and model diagnostics. Simply execute the cells with matplotlib code to see what you have achieved so far!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the data to see what it looks like\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(X[:, 0], y[:, 0], 'rx', markersize=10)\n",
    "plt.grid(True)\n",
    "plt.ylabel('Profit in $10,000s')\n",
    "plt.xlabel('Population of City in 10,000s')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear regression assumes that we can predict $y$ using a linear model (using matrix notation):\n",
    "\n",
    "$$ h_{\\theta}(x) = \\theta^Tx $$\n",
    "\n",
    "This is just a much more concise way to write:\n",
    "\n",
    "$$ h_{\\theta}(x) = \\theta_1 x_1 + \\theta_2 x_2 + ... \\theta_n x_n$$\n",
    "\n",
    "---\n",
    "#### Aside: Adding a Column of Ones to $X$ and the Expressing the Hypothesis Function in Vector Form\n",
    "\n",
    "(Ask us for help if anything is unclear.)\n",
    "\n",
    "How can we write the linear hypothesis in matrix form?\n",
    "\n",
    "$$ h_{\\theta}(x) = \\theta_0 + \\theta_1 x_1 + \\cdots + \\theta_n x_n. $$\n",
    "\n",
    "Recall that we can compactly write a set of linear equations as a matrix multiplication. For example\n",
    "\\begin{align}\n",
    "    \\begin{bmatrix}\n",
    "        a & b \\\\\n",
    "        c & d\n",
    "    \\end{bmatrix}\n",
    "    \\begin{bmatrix}\n",
    "        e & f \\\\\n",
    "        g & h\n",
    "    \\end{bmatrix} &=\n",
    "    \\begin{bmatrix}\n",
    "        ae + bg & af + bh \\\\\n",
    "        ce + dg & cf + dh\n",
    "    \\end{bmatrix} \\\\\n",
    "    \\begin{bmatrix}\n",
    "        i & j \\\\\n",
    "        k & l\n",
    "    \\end{bmatrix}\n",
    "    \\begin{bmatrix}\n",
    "        m \\\\\n",
    "        n\n",
    "    \\end{bmatrix} &=\n",
    "    \\begin{bmatrix}\n",
    "        im + jn \\\\\n",
    "        km + ln\n",
    "    \\end{bmatrix}\\\\\n",
    "    \\begin{bmatrix}\n",
    "        o & p\n",
    "    \\end{bmatrix}\n",
    "    \\begin{bmatrix}\n",
    "        q \\\\\n",
    "        r\n",
    "    \\end{bmatrix} &=\n",
    "    \\begin{bmatrix}\n",
    "        oq + pr\n",
    "    \\end{bmatrix}\n",
    "\\end{align}\n",
    "\n",
    "With this in mind, we can write the linear hypothesis as following:\n",
    "\\begin{align}\n",
    "    h_{\\theta}(x) = \\theta_0 + \\theta_1 x_1 + \\cdots + \\theta_n x_n =\n",
    "    \\begin{bmatrix}\n",
    "        \\theta_0 & \\theta_1 & \\cdots & \\theta_n\n",
    "    \\end{bmatrix}\n",
    "    \\begin{bmatrix}\n",
    "        1 \\\\\n",
    "        x_1 \\\\\n",
    "        \\vdots \\\\\n",
    "        x_n\n",
    "    \\end{bmatrix} =\n",
    "    \\theta^T x,\n",
    "\\end{align}\n",
    "where we add a one in the $x$ vector which you can think of as the $x_0$ term:\n",
    "\\begin{align}\n",
    "    x = \\begin{bmatrix}\n",
    "        1 \\\\\n",
    "        x_1 \\\\\n",
    "        \\vdots \\\\\n",
    "        x_n\n",
    "    \\end{bmatrix}.\n",
    "\\end{align}\n",
    "\n",
    "Now, the above for a single observation and we have $m$ of those. The convention is to write the matrix of inputs $X$ as a $m \\times (n + 1)$ matrix, where $n + 1$ is the number of variables *plus a constant term*:\n",
    "\\begin{align}\n",
    "    X = \\begin{bmatrix}\n",
    "        1 & x_1^{(1)} & \\cdots & x_n^{(1)} \\\\\n",
    "        1 & x_1^{(2)} & \\cdots & x_n^{(2)} \\\\\n",
    "        \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "        1 & x_1^{(m)} & \\cdots & x_n^{(m)}\n",
    "    \\end{bmatrix}.\n",
    "\\end{align}\n",
    "The first column is a column of ones such that when multiplied with the weights vector, we get the term $\\theta_0$ added for every observation:\n",
    "\\begin{align}\n",
    "    \\begin{bmatrix}\n",
    "        h_{\\theta}(x^{(1)}) \\\\\n",
    "        h_{\\theta}(x^{(2)}) \\\\\n",
    "        \\vdots \\\\\n",
    "        h_{\\theta}(x^{(m)})\n",
    "    \\end{bmatrix} =\n",
    "    \\begin{bmatrix}\n",
    "        \\theta_0 + \\theta_1 x_1^{(1)} + \\cdots + \\theta_n x_n^{(1)} \\\\\n",
    "        \\theta_0 + \\theta_1 x_1^{(2)} + \\cdots + \\theta_n x_n^{(2)} \\\\\n",
    "        \\vdots \\\\\n",
    "        \\theta_0 + \\theta_1 x_1^{(m)} + \\cdots + \\theta_n x_n^{(m)} \\\\\n",
    "    \\end{bmatrix} =\n",
    "    \\begin{bmatrix}\n",
    "        1 & x_1^{(1)} & \\cdots & x_n^{(1)} \\\\\n",
    "        1 & x_1^{(2)} & \\cdots & x_n^{(2)} \\\\\n",
    "        \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "        1 & x_1^{(m)} & \\cdots & x_n^{(m)}\n",
    "    \\end{bmatrix}\n",
    "    \\begin{bmatrix}\n",
    "        \\theta_0 \\\\\n",
    "        \\theta_1 \\\\\n",
    "        \\vdots \\\\\n",
    "        \\theta_n\n",
    "    \\end{bmatrix} =\n",
    "    X\\theta\n",
    "\\end{align}\n",
    "In the case of linear regression with a single variable, $X$ will have $ n + 1 = 2$ columns.\n",
    "\n",
    "For convenience, we will consider $n$ to be the number of columns of $X$ with the ones added."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = np.insert(X, 0, 1, axis=1)\n",
    "m, n = X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "The task of training a model is therefore finding parameters $\\theta$ such that $h_{\\theta}(x)$ is as close as possible to true labels $y$. What does it mean for a prediction and a true label to be 'close'? We have to define a cost function that penalises deviations of our predictions from the truth and then proceed to search for parameters that minimize such deivations.\n",
    "\n",
    "Linear regression models are trained using a *mean squared error* cost function:\n",
    "\n",
    "$$ J(\\theta) = \\frac{1}{2m} \\sum_{i = 1}^m \\left(h_{\\theta} (x^{(i)}) - y^{(i)} \\right)^2 $$\n",
    "\n",
    "where $m$ is the total number of examples in the training data.\n",
    "\n",
    "The function above simply says: the cost is equal to the sum of errors for all observations in the training data, where each error is equal to the squared difference between the prediction and the true value of $y$. This function is continuous and yields itself to optimization via gradient-based methods. The mathematically inclined may notice that this is a convex function and hence it is possible to find a global minimum using gradient descent (in fact, linear regression has a closed form solution, but we will focus on gradient descent as a more general method of training ML models).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "iterations = 1500\n",
    "alpha = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [